{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Preparation",
   "id": "39dc23429ab29cd5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load and explore data",
   "id": "f608904578fdf8c8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-01T11:57:11.592485Z",
     "start_time": "2025-11-01T11:57:11.312075Z"
    }
   },
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to C:\\Users\\MSI\n",
      "[nltk_data]     GF66\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to C:\\Users\\MSI\n",
      "[nltk_data]     GF66\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T10:32:40.665526Z",
     "start_time": "2025-11-01T10:32:39.469576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tagged_sentences = brown.tagged_sents()\n",
    "\n",
    "print(f\"Total sentences: {len(tagged_sentences)}\")"
   ],
   "id": "ee83dd2ed69c16c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 57340\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T10:32:45.063652Z",
     "start_time": "2025-11-01T10:32:43.647975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = [word.lower() for sent in tagged_sentences for word, tag in sent]\n",
    "vocab = set(words)\n",
    "print(f\"Vocab size: {len(vocab)}\")"
   ],
   "id": "57e9941fa207fc5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 49815\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T10:34:22.314326Z",
     "start_time": "2025-11-01T10:34:20.970853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tags = [tag for sent in tagged_sentences for word, tag in sent]\n",
    "tag_counts = Counter(tags)\n",
    "\n",
    "print(\"Tag distribution\")\n",
    "for tag, count in tag_counts.most_common(15):\n",
    "    print(f\"{tag}: {count}\")"
   ],
   "id": "d2960ff01f9be600",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag distribution\n",
      "NN: 152470\n",
      "IN: 120557\n",
      "AT: 97959\n",
      "JJ: 64028\n",
      ".: 60638\n",
      ",: 58156\n",
      "NNS: 55110\n",
      "CC: 37718\n",
      "RB: 36464\n",
      "NP: 34476\n",
      "VB: 33693\n",
      "VBN: 29186\n",
      "VBD: 26167\n",
      "CS: 22143\n",
      "PPS: 18253\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T10:34:33.169776Z",
     "start_time": "2025-11-01T10:34:33.158861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for sent in tagged_sentences[:5]:\n",
    "    print(sent)"
   ],
   "id": "af42cd3a1ae994ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]\n",
      "[('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')]\n",
      "[('The', 'AT'), ('September-October', 'NP'), ('term', 'NN'), ('jury', 'NN'), ('had', 'HVD'), ('been', 'BEN'), ('charged', 'VBN'), ('by', 'IN'), ('Fulton', 'NP-TL'), ('Superior', 'JJ-TL'), ('Court', 'NN-TL'), ('Judge', 'NN-TL'), ('Durwood', 'NP'), ('Pye', 'NP'), ('to', 'TO'), ('investigate', 'VB'), ('reports', 'NNS'), ('of', 'IN'), ('possible', 'JJ'), ('``', '``'), ('irregularities', 'NNS'), (\"''\", \"''\"), ('in', 'IN'), ('the', 'AT'), ('hard-fought', 'JJ'), ('primary', 'NN'), ('which', 'WDT'), ('was', 'BEDZ'), ('won', 'VBN'), ('by', 'IN'), ('Mayor-nominate', 'NN-TL'), ('Ivan', 'NP'), ('Allen', 'NP'), ('Jr.', 'NP'), ('.', '.')]\n",
      "[('``', '``'), ('Only', 'RB'), ('a', 'AT'), ('relative', 'JJ'), ('handful', 'NN'), ('of', 'IN'), ('such', 'JJ'), ('reports', 'NNS'), ('was', 'BEDZ'), ('received', 'VBN'), (\"''\", \"''\"), (',', ','), ('the', 'AT'), ('jury', 'NN'), ('said', 'VBD'), (',', ','), ('``', '``'), ('considering', 'IN'), ('the', 'AT'), ('widespread', 'JJ'), ('interest', 'NN'), ('in', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('the', 'AT'), ('number', 'NN'), ('of', 'IN'), ('voters', 'NNS'), ('and', 'CC'), ('the', 'AT'), ('size', 'NN'), ('of', 'IN'), ('this', 'DT'), ('city', 'NN'), (\"''\", \"''\"), ('.', '.')]\n",
      "[('The', 'AT'), ('jury', 'NN'), ('said', 'VBD'), ('it', 'PPS'), ('did', 'DOD'), ('find', 'VB'), ('that', 'CS'), ('many', 'AP'), ('of', 'IN'), (\"Georgia's\", 'NP$'), ('registration', 'NN'), ('and', 'CC'), ('election', 'NN'), ('laws', 'NNS'), ('``', '``'), ('are', 'BER'), ('outmoded', 'JJ'), ('or', 'CC'), ('inadequate', 'JJ'), ('and', 'CC'), ('often', 'RB'), ('ambiguous', 'JJ'), (\"''\", \"''\"), ('.', '.')]\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Data splitting",
   "id": "f847ce34774db450"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T10:34:52.361388Z",
     "start_time": "2025-11-01T10:34:40.879317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_dev_data = train_test_split(tagged_sentences, test_size=0.2, random_state=42)\n",
    "\n",
    "test_data, dev_data = train_test_split(test_dev_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training data size: {len(train_data)}\")\n",
    "print(f\"Test data size: {len(test_data)}\")\n",
    "print(f\"Dev data size: {len(dev_data)}\")"
   ],
   "id": "68924bf8ccb0ac9c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 45872\n",
      "Test data size: 5734\n",
      "Dev data size: 5734\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Data preprocessing",
   "id": "c663544c0801591d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T10:34:55.655982Z",
     "start_time": "2025-11-01T10:34:55.644009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_data(data):\n",
    "    processed_data = [[(word.lower(), tag) for (word, tag) in sent] for sent in data]\n",
    "\n",
    "    vocab = sorted({word for sent in processed_data for (word, tag) in sent})\n",
    "    tags = sorted({tag for sent in processed_data for (word, tag) in sent})\n",
    "\n",
    "    word_idx = {word: i for i, word in enumerate(vocab)}\n",
    "    tag_idx = {tag: i for i, tag in enumerate(tags)}\n",
    "    idx_word = {i: word for word, i in word_idx.items()}\n",
    "    idx_tag = {i: tag for tag, i in tag_idx.items()}\n",
    "\n",
    "    return processed_data, vocab, tags, word_idx, tag_idx, idx_word, idx_tag"
   ],
   "id": "d1f942a577a952df",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T10:37:05.421010Z",
     "start_time": "2025-11-01T10:37:04.661480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data, vocab, tags, word_idx, tag_idx, idx_word, idx_tag = preprocess_data(train_data)\n",
    "\n",
    "print(\"Number of sentences:\", len(train_data))\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "print(\"Number of tags:\", len(tags))\n",
    "print(\"Sample vocab:\", vocab[10])\n",
    "print(\"Sample tags:\", tags[250:260])"
   ],
   "id": "2a4067bd20eea9c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 45872\n",
      "Vocabulary size: 45157\n",
      "Number of tags: 456\n",
      "Sample vocab: ['!', '$.027', '$.03', '$.054/mbf', '$.07', '$.07/cwt', '$.076', '$.09', '$.10-a-minute', '$.105']\n",
      "Sample tags: ['NN$-HL', 'NN$-TL', 'NN+BEZ', 'NN+HVD-TL', 'NN+HVZ', 'NN+IN', 'NN+MD', 'NN+NN-NC', 'NN-HL', 'NN-NC']\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### HMM Implementation",
   "id": "28ffa39dace077a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Initial probabilities",
   "id": "37c12c6b549bb435"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T13:55:15.365707Z",
     "start_time": "2025-11-01T13:55:15.355823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initial_probabailities(training_data, all_tags):\n",
    "    counts = {tag: 1 for tag in all_tags}\n",
    "    total = len(training_data) + len(all_tags)\n",
    "\n",
    "    for sentence in training_data:\n",
    "        first_tag = sentence[0][1]\n",
    "        counts[first_tag] += 1\n",
    "\n",
    "    initial_prob = {tag: counts[tag]/total for tag in all_tags}\n",
    "\n",
    "    return initial_prob"
   ],
   "id": "833326030c413a96",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T13:55:17.262487Z",
     "start_time": "2025-11-01T13:55:17.084496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "initial_probs = initial_probabailities(train_data, tags)\n",
    "\n",
    "print(\"Sample initial probabilities:\")\n",
    "for tag in [\"NN\", \"VB\", \"DT\"]:\n",
    "    print(tag, initial_probs.get(tag, None))"
   ],
   "id": "a46b1c42fdd77542",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample initial probabilities:\n",
      "NN 0.024218615092384734\n",
      "VB 0.01489380072526334\n",
      "DT 0.02767225004317044\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Transition probabilities",
   "id": "36e1f530888a15ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T14:12:44.668915Z",
     "start_time": "2025-11-01T14:12:44.659387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def transition_probabilities(training_data, all_tags):\n",
    "    start_tag, end_tag = \"<START>\", \"<END>\"\n",
    "    tags_with_start_end = all_tags + [start_tag, end_tag]\n",
    "\n",
    "    tag_counts = {tag: 0 for tag in tags_with_start_end}\n",
    "    transition_counts = {t1: {t2: 1 for t2 in tags_with_start_end} for t1 in tags_with_start_end}\n",
    "\n",
    "    for sentence in training_data:\n",
    "        tags = [start_tag] + [tag for _, tag in sentence] + [end_tag]\n",
    "        for i in range(1, len(tags)):\n",
    "            prev_tag = tags[i-1]\n",
    "            curr_tag = tags[i]\n",
    "            transition_counts[prev_tag][curr_tag] += 1\n",
    "            tag_counts[prev_tag] += 1\n",
    "\n",
    "    transition_probs = {}\n",
    "    for prev_tag, curr_tag in transition_counts.items():\n",
    "        transition_probs[prev_tag] = {}\n",
    "        for next_tag, count in curr_tag.items():\n",
    "            transition_probs[prev_tag][next_tag] = count / (tag_counts[prev_tag] + len(tags_with_start_end))\n",
    "\n",
    "    return transition_probs"
   ],
   "id": "563a37c66d9e5ed7",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T14:12:47.905186Z",
     "start_time": "2025-11-01T14:12:47.446557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_tags = list({tag for sent in train_data for _, tag in sent})\n",
    "\n",
    "transition_probs = transition_probabilities(train_data, all_tags)\n",
    "\n",
    "print(\"Number of tags:\", len(all_tags))\n",
    "print(\"Sample transition probabilities:\")\n",
    "print(\"P(VB|NN):\", transition_probs[\"VB\"][\"NN\"])\n",
    "print(\"P(END|NN):\", transition_probs[\"<END>\"][\"NN\"])"
   ],
   "id": "5108149f6735b947",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tags: 456\n",
      "Sample transition probabilities:\n",
      "P(VB|NN): 0.0428011326508386\n",
      "P(END|NN): 0.002183406113537118\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Emission probabilities",
   "id": "dd38a2397f1d8528"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T13:04:06.760506Z",
     "start_time": "2025-11-01T13:04:06.740768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def emission_probabilities(training_data, all_tags):\n",
    "    vocab = list({word.lower() for sent in training_data for word, _ in sent})\n",
    "\n",
    "    emission_counts = {tag: {word: 1 for word in vocab} for tag in all_tags}\n",
    "    tag_counts = {tag: 0 for tag in all_tags}\n",
    "\n",
    "    for sentence in training_data:\n",
    "        for word, tag in sentence:\n",
    "            word = word.lower()\n",
    "            emission_counts[tag][word] += 1\n",
    "            tag_counts[tag] += 1\n",
    "\n",
    "    emission_probs = {}\n",
    "    for tag in all_tags:\n",
    "        total = tag_counts[tag] + len(vocab)\n",
    "        emission_probs[tag] = {}\n",
    "        for word, count in emission_counts[tag].items():\n",
    "            emission_probs[tag][word] = count / total\n",
    "\n",
    "    emission_probs[\"<OOV>\"] = {tag: 1e-6 for tag in all_tags}\n",
    "\n",
    "    return emission_probs"
   ],
   "id": "c649e41b942f9e75",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T13:15:28.059573Z",
     "start_time": "2025-11-01T13:15:28.033583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if \"cases\" in vocab and \"problem\" in vocab:\n",
    "    print(True)\n",
    "\n",
    "if \"outofvocabword\" not in vocab:\n",
    "    print(True)\n"
   ],
   "id": "eb82665238efc3b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T13:11:32.900934Z",
     "start_time": "2025-11-01T13:11:27.232114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "emission_probs = emission_probabilities(train_data, all_tags)\n",
    "print(\"Sample emission probabilities:\")\n",
    "print(\"P(cases|NNS):\", emission_probs[\"NNS\"][\"cases\"])\n",
    "print(\"P(problem|NN):\", emission_probs[\"NN\"][\"problem\"])\n",
    "\n",
    "print(\"P(outofvocabword|NN):\", emission_probs[\"NN\"].get(\"outofvocabword\", emission_probs[\"<OOV>\"][\"NN\"]))"
   ],
   "id": "e76aeb597b5fd2c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample emission probabilities:\n",
      "P(cases|NNS): 0.0013303075358009234\n",
      "P(problem|NN): 0.0015022743595882212\n",
      "P(outofvocabword|NN): 1e-06\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "HMM POS Tagger",
   "id": "5f65150422bf0590"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T14:13:00.484020Z",
     "start_time": "2025-11-01T14:13:00.473703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "class HMMPOSTagger:\n",
    "    def __init__(self, all_tags, initial_probs, transition_probs, emission_probs):\n",
    "        self.POS = all_tags\n",
    "        self.initial_prob = initial_probs\n",
    "        self.transition_prob = transition_probs\n",
    "        self.emission_prob = emission_probs\n",
    "\n",
    "    def viterbi_decode(self, sentence):\n",
    "        V = []\n",
    "        backpointer = []\n",
    "\n",
    "        first_probs = {}\n",
    "        first_back = {}\n",
    "        for tag in self.POS:\n",
    "            emission = self.emission_prob[tag].get(sentence[0], self.emission_prob[\"<OOV>\"][tag])\n",
    "            first_probs[tag] = math.log(self.initial_prob[tag]) + math.log(emission)\n",
    "            first_back[tag] = None\n",
    "\n",
    "        V.append(first_probs)\n",
    "        backpointer.append(first_back)\n",
    "\n",
    "        for i in range(1, len(sentence)):\n",
    "            current_probs = {}\n",
    "            current_back = {}\n",
    "            word = sentence[i]\n",
    "            for tag in self.POS:\n",
    "                max_prob = float(\"-inf\")\n",
    "                best_prev_tag = None\n",
    "                for prev_tag in self.POS:\n",
    "                    transition = self.transition_prob[prev_tag].get(tag, 1e-6)\n",
    "                    emission = self.emission_prob[tag].get(word, self.emission_prob[\"<OOV>\"][tag])\n",
    "                    prob = V[i-1][prev_tag] + math.log(transition) + math.log(emission)\n",
    "                    if prob > max_prob:\n",
    "                        max_prob = prob\n",
    "                        best_prev_tag = prev_tag\n",
    "                current_probs[tag] = max_prob\n",
    "                current_back[tag] = best_prev_tag\n",
    "            V.append(current_probs)\n",
    "            backpointer.append(current_back)\n",
    "\n",
    "        best_last_tag = max(V[-1], key=V[-1].get)\n",
    "        best_tags = [best_last_tag]\n",
    "        for i in range(len(sentence)-1, 0, -1):\n",
    "            best_tags.append(backpointer[i][best_tags[-1]])\n",
    "        best_tags.reverse()\n",
    "\n",
    "        return best_tags"
   ],
   "id": "e6a05002e16a6d5e",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T14:15:18.154902Z",
     "start_time": "2025-11-01T14:15:13.968455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hmm_tagger = HMMPOSTagger(all_tags, initial_probs, transition_probs, emission_probs)\n",
    "\n",
    "test_sentences = [\n",
    "    [\"The\", \"tiny\", \"sparrow\", \"quickly\", \"flew\", \"over\", \"the\", \"garden\", \"fence\"],\n",
    "    [\"After\", \"a\", \"long\", \"day\", \"of\", \"work\", \"she\", \"decided\", \"to\", \"read\", \"a\", \"book\"],\n",
    "    [\"Scientists\", \"have\", \"recently\", \"discovered\", \"a\", \"new\", \"species\", \"of\", \"frog\", \"in\", \"the\", \"Amazon\"],\n",
    "    [\"The\", \"team\", \"celebrated\", \"their\", \"hard-earned\", \"victory\", \"with\", \"a\", \"grand\", \"dinner\"],\n",
    "    [\"In\", \"spite\", \"of\", \"the\", \"heavy\", \"rain\", \"the\", \"festival\", \"continued\", \"until\", \"late\", \"at\", \"night\"]\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    predicted_tags = hmm_tagger.viterbi_decode([w.lower() for w in sentence])\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Predicted Tags:\", predicted_tags)"
   ],
   "id": "2e22ac1d615a7763",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['The', 'tiny', 'sparrow', 'quickly', 'flew', 'over', 'the', 'garden', 'fence']\n",
      "Predicted Tags: ['AT', 'JJ', 'NN', 'RB', 'VBD', 'IN', 'AT', 'NN', 'NN']\n",
      "Sentence: ['After', 'a', 'long', 'day', 'of', 'work', 'she', 'decided', 'to', 'read', 'a', 'book']\n",
      "Predicted Tags: ['IN', 'AT', 'JJ', 'NN', 'IN', 'NN', 'PPS', 'VBD', 'TO', 'VB', 'AT', 'NN']\n",
      "Sentence: ['Scientists', 'have', 'recently', 'discovered', 'a', 'new', 'species', 'of', 'frog', 'in', 'the', 'Amazon']\n",
      "Predicted Tags: ['PPSS', 'HV', 'RB', 'VBD', 'AT', 'JJ', 'NN', 'IN', 'NN', 'IN', 'AT', 'NN']\n",
      "Sentence: ['The', 'team', 'celebrated', 'their', 'hard-earned', 'victory', 'with', 'a', 'grand', 'dinner']\n",
      "Predicted Tags: ['AT', 'NN', 'IN', 'PP$', 'JJ', 'NN', 'IN', 'AT', 'JJ', 'NN']\n",
      "Sentence: ['In', 'spite', 'of', 'the', 'heavy', 'rain', 'the', 'festival', 'continued', 'until', 'late', 'at', 'night']\n",
      "Predicted Tags: ['IN', 'NN', 'IN', 'AT', 'JJ', 'NN', 'AT', 'NN', 'VBD', 'IN', 'JJ', 'IN', 'NN']\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Evaluation",
   "id": "88bd6ecfb622cc36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T16:18:51.309566Z",
     "start_time": "2025-11-01T15:55:17.042055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_tagger(tagger, data):\n",
    "    total_words = 0\n",
    "    correct_tags = 0\n",
    "\n",
    "    for sentence in data:\n",
    "        words = [w.lower() for w, t in sentence]\n",
    "        original_tags = [t for w, t in sentence]\n",
    "\n",
    "        predicted_tags = tagger.viterbi_decode(words)\n",
    "\n",
    "        total_words += len(words)\n",
    "        correct_tags += sum([p == g for p, g in zip(predicted_tags, original_tags)])\n",
    "\n",
    "    accuracy = correct_tags / total_words\n",
    "    return accuracy\n",
    "\n",
    "dev_accuracy = evaluate_tagger(hmm_tagger, dev_data[:1000])\n",
    "print(f\"Dev set accuracy: {dev_accuracy:.4f}\")"
   ],
   "id": "3abd5a3b259905f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev set accuracy: 0.8900\n"
     ]
    }
   ],
   "execution_count": 93
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
