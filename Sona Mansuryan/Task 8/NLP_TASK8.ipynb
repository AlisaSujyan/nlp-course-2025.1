{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# **Hidden Markov Model (HMM) Part-of-Speech Tagger**\n",
    "# **Implementation with Brute Force and Viterbi Decoding**\n",
    "\n"
   ],
   "metadata": {
    "id": "JmexklSflqRK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. Import Required Libraries**"
   ],
   "metadata": {
    "id": "TmwB2y4OlvYo"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HgHdpGEjlpQK",
    "outputId": "420088c4-0d27-4b05-ac9b-626d2fdd9741"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ Libraries loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print(\"✓ Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. Training Data**"
   ],
   "metadata": {
    "id": "jVOO1ZcdmA85"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This training data is used to train the HMM model.\n",
    "Each sentence is a list of (word, tag) tuples."
   ],
   "metadata": {
    "id": "dRG0pOE2mLbQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "training_data = [\n",
    "    [(\"The\", \"DET\"), (\"dog\", \"NOUN\"), (\"barks\", \"VERB\")],\n",
    "    [(\"The\", \"DET\"), (\"cat\", \"NOUN\"), (\"meows\", \"VERB\")],\n",
    "    [(\"A\", \"DET\"), (\"dog\", \"NOUN\"), (\"runs\", \"VERB\")],\n",
    "    [(\"The\", \"DET\"), (\"dog\", \"NOUN\"), (\"runs\", \"VERB\")],\n",
    "    [(\"A\", \"DET\"), (\"cat\", \"NOUN\"), (\"sleeps\", \"VERB\")],\n",
    "    [(\"The\", \"DET\"), (\"big\", \"ADJ\"), (\"dog\", \"NOUN\"), (\"barks\", \"VERB\")],\n",
    "    [(\"A\", \"DET\"), (\"small\", \"ADJ\"), (\"cat\", \"NOUN\"), (\"meows\", \"VERB\")],\n",
    "    [(\"The\", \"DET\"), (\"dog\", \"NOUN\"), (\"sleeps\", \"VERB\")],\n",
    "    [(\"The\", \"DET\"), (\"cat\", \"NOUN\"), (\"runs\", \"VERB\")],\n",
    "    [(\"A\", \"DET\"), (\"big\", \"ADJ\"), (\"dog\", \"NOUN\"), (\"sleeps\", \"VERB\")],\n",
    "    [(\"The\", \"DET\"), (\"small\", \"ADJ\"), (\"cat\", \"NOUN\"), (\"runs\", \"VERB\")],\n",
    "    [(\"Dogs\", \"NOUN\"), (\"bark\", \"VERB\")],\n",
    "    [(\"Cats\", \"NOUN\"), (\"meow\", \"VERB\")],\n",
    "    [(\"The\", \"DET\"), (\"dog\", \"NOUN\"), (\"in\", \"ADP\"), (\"the\", \"DET\"), (\"house\", \"NOUN\"), (\"barks\", \"VERB\")],\n",
    "    [(\"A\", \"DET\"), (\"cat\", \"NOUN\"), (\"on\", \"ADP\"), (\"the\", \"DET\"), (\"mat\", \"NOUN\"), (\"sleeps\", \"VERB\")],\n",
    "]\n",
    "\n",
    "print(f\"✓ Training data loaded: {len(training_data)} sentences\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "moE8kjo1mEVq",
    "outputId": "43ba87a6-e18c-4915-815a-afbd3fd63cd1"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ Training data loaded: 15 sentences\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3. Extract Unique POS Tags**"
   ],
   "metadata": {
    "id": "qupmxxIWNH82"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract all unique POS tags that appear in the training corpus.\n",
    "These tags will serve as the hidden states in our HMM."
   ],
   "metadata": {
    "id": "IZ0i4eUnSFfG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tags_set = set()\n",
    "for sent in training_data:\n",
    "    for word_tag_pair in sent:\n",
    "        tags_set.add(word_tag_pair[1])\n",
    "\n",
    "POS_list = sorted(list(tags_set))\n",
    "print(\"POS Tags:\", POS_list)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bw16Npd1NITk",
    "outputId": "845c6f11-9cef-4f36-dff6-5fc092bd520e"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "POS Tags: ['ADJ', 'ADP', 'DET', 'NOUN', 'VERB']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**4. Initial Probabilities**"
   ],
   "metadata": {
    "id": "x_0Npcf_Ns76"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This section computes the Initial Probabilities of each POS tag — the likelihood that a tag appears at the beginning of a sentence.\n",
    "\n",
    "Since some tags may not occur at the start in the training data, Laplace (Add-One) smoothing is applied to avoid zero probabilities.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$P(\\text{tag}) = \\frac{\\text{count}(\\text{tag at start}) + 1}{\\text{total sentences} + \\text{num tags}}$$\n",
    "\n",
    "**Explanation of terms:**\n",
    "\n",
    "* **`count(tag at start)`** → number of sentences that start with this tag,\n",
    "\n",
    "* **`total sentences`** → total number of sentences in the corpus,\n",
    "\n",
    "* **`num tags`** → total number of unique POS tags.\n",
    "\n",
    "\n",
    "The result is a probability distribution over all tags, representing how likely each tag is to appear as the first one in a sentence.\n",
    "\n",
    "These probabilities define the initial state distribution of the Hidden Markov Model (HMM), serving as the starting point for predicting tag sequences."
   ],
   "metadata": {
    "id": "bSwtIzbIR-nK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_initial_probabilities(data):\n",
    "    start_tag_counts = {}\n",
    "\n",
    "    for tag in POS_list:\n",
    "        start_tag_counts[tag] = 1\n",
    "\n",
    "    for sentence in data:\n",
    "        first_tag = sentence[0][1]\n",
    "        start_tag_counts[first_tag] += 1\n",
    "\n",
    "    total = len(data) + len(POS_list)\n",
    "    probabilities = {}\n",
    "\n",
    "    for tag in POS_list:\n",
    "        probabilities[tag] = start_tag_counts[tag] / total\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "initial_probabilities = compute_initial_probabilities(training_data)\n",
    "print(\"Initial Probabilities:\", initial_probabilities)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qNW4Y5KCNtUc",
    "outputId": "4b72b07e-8d38-416e-cd8e-db828ef0454f"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial Probabilities: {'ADJ': 0.05, 'ADP': 0.05, 'DET': 0.7, 'NOUN': 0.15, 'VERB': 0.05}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**5. Count Tag Occurrences**"
   ],
   "metadata": {
    "id": "Sjo1ZYSHO7bC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Count total occurrences of each tag in the training data.\n",
    "Needed for normalizing transition and emission probabilities."
   ],
   "metadata": {
    "id": "GTcqw00tSCk1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def count_tags(data):\n",
    "    counts = {}\n",
    "    for tag in POS_list:\n",
    "        counts[tag] = 0\n",
    "\n",
    "    for sentence in data:\n",
    "        for _, tag in sentence:\n",
    "            counts[tag] += 1\n",
    "\n",
    "    return counts\n",
    "\n",
    "tag_frequencies = count_tags(training_data)\n",
    "print(\"Tag Frequencies:\", tag_frequencies)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hn9Pk0fIO7x3",
    "outputId": "808ddb9c-639f-409b-f776-2f1119077e6c"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tag Frequencies: {'ADJ': 4, 'ADP': 2, 'DET': 15, 'NOUN': 17, 'VERB': 15}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**6. Transition Probabilities**"
   ],
   "metadata": {
    "id": "pW5QSffjPl1E"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This section computes the Transition Probabilities, which represent the likelihood of moving from one POS tag to another in a sentence.\n",
    "\n",
    "In the context of an HMM, these probabilities model the dependency between consecutive tags — for example, how likely it is that a NOUN is followed by a VERB.\n",
    "\n",
    "Since some tag transitions may never appear in the training data, Laplace (Add-One) smoothing is applied to ensure that every possible transition receives a non-zero probability.\n",
    "\n",
    "**Formula:** $$P(\\text{tag}_i \\mid \\text{tag}_{i-1}) = \\frac{\\text{count}(\\text{tag}_{i-1}, \\text{tag}_i) + 1}{\\text{count}(\\text{tag}_{i-1}) + \\text{num tags}}$$\n",
    "\n",
    "**Explanation of terms:**\n",
    "\n",
    "* **`count(tag_{i-1}, tag_i)`** → number of times the pair of consecutive tags occurs in the data,\n",
    "\n",
    "* **`count(tag_{i-1})`** → number of occurrences of the previous tag,\n",
    "\n",
    "* **`num tags`** → total number of unique POS tags.\n",
    "\n",
    "\n",
    "The resulting transition matrix provides a full mapping of how likely each tag is to follow another.\n",
    "\n",
    "These probabilities form a core component of the HMM structure, as they describe the relationships between hidden states and are later used by the Viterbi algorithm to predict the most probable tag sequence."
   ],
   "metadata": {
    "id": "AcUfoBVNR7BU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_transition_probabilities(data, tag_counts):\n",
    "    transitions = {}\n",
    "\n",
    "    for tag1 in POS_list:\n",
    "        transitions[tag1] = {}\n",
    "        for tag2 in POS_list:\n",
    "            transitions[tag1][tag2] = 1\n",
    "\n",
    "    for sentence in data:\n",
    "        for idx in range(len(sentence) - 1):\n",
    "            prev_tag = sentence[idx][1]\n",
    "            next_tag = sentence[idx + 1][1]\n",
    "            transitions[prev_tag][next_tag] += 1\n",
    "\n",
    "    for prev_tag in POS_list:\n",
    "        denominator = tag_counts[prev_tag] + len(POS_list)\n",
    "        for curr_tag in POS_list:\n",
    "            transitions[prev_tag][curr_tag] /= denominator\n",
    "\n",
    "    return transitions\n",
    "\n",
    "transition_probabilities = compute_transition_probabilities(training_data, tag_frequencies)\n",
    "\n",
    "print(\"Transition Probabilities:\")\n",
    "for tag1, trans_dict in transition_probabilities.items():\n",
    "    formatted_trans = {t2: f\"{p:.2f}\" for t2, p in trans_dict.items()}\n",
    "    print(f\"  '{tag1}': {formatted_trans}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36vFuhM4P1-h",
    "outputId": "ca5f7a9e-af3c-4dc7-eccf-85aa9be84a2e"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Transition Probabilities:\n",
      "  'ADJ': {'ADJ': '0.11', 'ADP': '0.11', 'DET': '0.11', 'NOUN': '0.56', 'VERB': '0.11'}\n",
      "  'ADP': {'ADJ': '0.14', 'ADP': '0.14', 'DET': '0.43', 'NOUN': '0.14', 'VERB': '0.14'}\n",
      "  'DET': {'ADJ': '0.25', 'ADP': '0.05', 'DET': '0.05', 'NOUN': '0.60', 'VERB': '0.05'}\n",
      "  'NOUN': {'ADJ': '0.05', 'ADP': '0.14', 'DET': '0.05', 'NOUN': '0.05', 'VERB': '0.73'}\n",
      "  'VERB': {'ADJ': '0.05', 'ADP': '0.05', 'DET': '0.05', 'NOUN': '0.05', 'VERB': '0.05'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**7․ Extract Vocabulary**\n",
    "\n",
    "Extract all unique words from the training data to build vocabulary."
   ],
   "metadata": {
    "id": "v9Ay8yFWQSaa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_vocabulary(data):\n",
    "    words = []\n",
    "    for sentence in data:\n",
    "        for word, _ in sentence:\n",
    "            if word not in words:\n",
    "                words.append(word)\n",
    "    return words\n",
    "\n",
    "vocabulary = extract_vocabulary(training_data)\n",
    "print(\"Vocabulary:\", vocabulary)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lfNkbYIXQSv4",
    "outputId": "3b2e1e89-76cf-4425-a504-cbd1e729dbbb"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary: ['The', 'dog', 'barks', 'cat', 'meows', 'A', 'runs', 'sleeps', 'big', 'small', 'Dogs', 'bark', 'Cats', 'meow', 'in', 'the', 'house', 'on', 'mat']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**8. Emission Probabilities**\n",
    "\n"
   ],
   "metadata": {
    "id": "ggom2LQKRD_h"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This section computes the Emission Probabilities, which represent the likelihood of a word being generated (emitted) from a specific POS tag.\n",
    "\n",
    "In an HMM-based POS tagger, emission probabilities define the relationship between hidden states (tags) and observed words.\n",
    "They tell the model how likely it is to see a word given that the current hidden state (tag) is known.\n",
    "\n",
    "Because some word-tag combinations might never appear in the training data, Laplace (Add-One) smoothing is applied to avoid zero probabilities.\n",
    "\n",
    "**Formula:** $$P(\\text{word} \\mid \\text{tag}) = \\frac{\\text{count}(\\text{word}, \\text{tag}) + 1}{\\text{count}(\\text{tag}) + \\text{num words}}$$\n",
    "\n",
    "**Explanation of terms:**\n",
    "\n",
    "* **`count(word, tag)`** → number of times a specific word appears with a given tag,\n",
    "\n",
    "* **`count(tag)`** → total occurrences of the tag in the corpus,\n",
    "\n",
    "* **`num words`** → total number of unique words in the vocabulary.\n",
    "\n",
    "These probabilities form the observation model of the HMM, allowing it to estimate which words are most characteristic of each tag when predicting unseen sentences."
   ],
   "metadata": {
    "id": "tPYxTpLJRmfR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_emission_probabilities(data, tag_counts, vocab):\n",
    "    emissions = {}\n",
    "\n",
    "    for tag in POS_list:\n",
    "        emissions[tag] = {}\n",
    "        for word in vocab:\n",
    "            emissions[tag][word] = 1\n",
    "\n",
    "    for sentence in data:\n",
    "        for word, tag in sentence:\n",
    "            emissions[tag][word] += 1\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "    for tag in POS_list:\n",
    "        denominator = tag_counts[tag] + vocab_size\n",
    "        for word in vocab:\n",
    "            emissions[tag][word] /= denominator\n",
    "\n",
    "    return emissions\n",
    "\n",
    "emission_probabilities = compute_emission_probabilities(training_data, tag_frequencies, vocabulary)\n",
    "\n",
    "print(\"Emission Probabilities:\")\n",
    "for tag, emission_dict in emission_probabilities.items():\n",
    "    formatted_probs = {word: f\"{prob:.2f}\" for word, prob in emission_dict.items()}\n",
    "    print(f\"  '{tag}': {formatted_probs}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sx91LwFiREWo",
    "outputId": "d078fffc-a6f9-459d-8bae-8e8685e9263c"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Emission Probabilities:\n",
      "  'ADJ': {'The': '0.04', 'dog': '0.04', 'barks': '0.04', 'cat': '0.04', 'meows': '0.04', 'A': '0.04', 'runs': '0.04', 'sleeps': '0.04', 'big': '0.13', 'small': '0.13', 'Dogs': '0.04', 'bark': '0.04', 'Cats': '0.04', 'meow': '0.04', 'in': '0.04', 'the': '0.04', 'house': '0.04', 'on': '0.04', 'mat': '0.04'}\n",
      "  'ADP': {'The': '0.05', 'dog': '0.05', 'barks': '0.05', 'cat': '0.05', 'meows': '0.05', 'A': '0.05', 'runs': '0.05', 'sleeps': '0.05', 'big': '0.05', 'small': '0.05', 'Dogs': '0.05', 'bark': '0.05', 'Cats': '0.05', 'meow': '0.05', 'in': '0.10', 'the': '0.05', 'house': '0.05', 'on': '0.10', 'mat': '0.05'}\n",
      "  'DET': {'The': '0.26', 'dog': '0.03', 'barks': '0.03', 'cat': '0.03', 'meows': '0.03', 'A': '0.18', 'runs': '0.03', 'sleeps': '0.03', 'big': '0.03', 'small': '0.03', 'Dogs': '0.03', 'bark': '0.03', 'Cats': '0.03', 'meow': '0.03', 'in': '0.03', 'the': '0.09', 'house': '0.03', 'on': '0.03', 'mat': '0.03'}\n",
      "  'NOUN': {'The': '0.03', 'dog': '0.22', 'barks': '0.03', 'cat': '0.19', 'meows': '0.03', 'A': '0.03', 'runs': '0.03', 'sleeps': '0.03', 'big': '0.03', 'small': '0.03', 'Dogs': '0.06', 'bark': '0.03', 'Cats': '0.06', 'meow': '0.03', 'in': '0.03', 'the': '0.03', 'house': '0.06', 'on': '0.03', 'mat': '0.06'}\n",
      "  'VERB': {'The': '0.03', 'dog': '0.03', 'barks': '0.12', 'cat': '0.03', 'meows': '0.09', 'A': '0.03', 'runs': '0.15', 'sleeps': '0.15', 'big': '0.03', 'small': '0.03', 'Dogs': '0.03', 'bark': '0.06', 'Cats': '0.03', 'meow': '0.06', 'in': '0.03', 'the': '0.03', 'house': '0.03', 'on': '0.03', 'mat': '0.03'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**9. HMM POS Tagger Class**"
   ],
   "metadata": {
    "id": "j3yCie0yZoFW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**HMMPOSTagger (partial) – Key Points**\n",
    "\n",
    "* **`__init__`**: Initializes empty structures for tags, vocabulary, counts, and probabilities.\n",
    "* **`train(data)`**: Fills `tag_set`, `word_set`, counts, and computes initial (`pi`), transition (`A`), and emission (`B`) probabilities using Add-One smoothing.\n",
    "* **`recursive_generate_sequences(seq, pos, sent_len, result)`**: Recursively generates all possible tag sequences of length `sent_len` (for brute-force decoding).\n",
    "* **`brute_force_decode(sentence)`**: Finds the most probable tag sequence by checking all possible sequences:\n",
    "\n",
    "\n",
    "$$ P(\\text{tags, words}) = P(\\text{tag}_1) \\cdot P(\\text{word}_1 \\mid \\text{tag}*1) \\cdot \\prod*{i=2}^{n} P(\\text{tag}*i \\mid \\text{tag}*{i-1}) \\cdot P(\\text{word}_i \\mid \\text{tag}_i) $$\n",
    "\n",
    "\n",
    "* Uses a small default probability $( 1 \\times 10^{-6} )$  for unknown words.\n"
   ],
   "metadata": {
    "id": "Fb56pmGLZv96"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "class HMMPOSTagger:\n",
    "    def __init__(self):\n",
    "        self.tag_set = []\n",
    "        self.word_set = []\n",
    "        self.pi = {}\n",
    "        self.A = {}\n",
    "        self.B = {}\n",
    "        self.tag_totals = {}\n",
    "\n",
    "    def train(self, data):\n",
    "        all_tags = set()\n",
    "        for sentence in data:\n",
    "            for _, tag in sentence:\n",
    "                all_tags.add(tag)\n",
    "        self.tag_set = sorted(list(all_tags))\n",
    "\n",
    "        self.word_set = extract_vocabulary(data)\n",
    "        self.tag_totals = count_tags(data)\n",
    "        self.pi = compute_initial_probabilities(data)\n",
    "        self.A = compute_transition_probabilities(data, self.tag_totals)\n",
    "        self.B = compute_emission_probabilities(data, self.tag_totals, self.word_set)\n",
    "\n",
    "    def recursive_generate_sequences(self, seq, pos, sent_len, result):\n",
    "        if pos == sent_len:\n",
    "            result.append(seq[:])\n",
    "            return\n",
    "\n",
    "        for t in self.tag_set:\n",
    "            seq.append(t)\n",
    "            self.recursive_generate_sequences(seq, pos + 1, sent_len, result)\n",
    "            seq.pop()\n",
    "\n",
    "    def brute_force_decode(self, sentence):\n",
    "        all_tag_sequences = []\n",
    "        self.recursive_generate_sequences([], 0, len(sentence), all_tag_sequences)\n",
    "\n",
    "        max_probability = 0\n",
    "        optimal_sequence = None\n",
    "\n",
    "        for tag_seq in all_tag_sequences:\n",
    "            prob = self.pi[tag_seq[0]]\n",
    "            prob *= self.B[tag_seq[0]].get(sentence[0], 1e-6)\n",
    "\n",
    "            for pos in range(1, len(sentence)):\n",
    "                prob *= self.A[tag_seq[pos - 1]][tag_seq[pos]]\n",
    "                prob *= self.B[tag_seq[pos]].get(sentence[pos], 1e-6)\n",
    "\n",
    "            if prob > max_probability:\n",
    "                max_probability = prob\n",
    "                optimal_sequence = tag_seq\n",
    "\n",
    "        return optimal_sequence"
   ],
   "metadata": {
    "id": "NIPRyxQaZorU"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**10. Viterbi Decoding Function**"
   ],
   "metadata": {
    "id": "ZEVYWpnUhBi6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `viterbi_decode` function finds the most probable POS tag sequence for a given sentence using the **Viterbi algorithm**, an efficient dynamic programming approach.\n",
    "\n",
    "**Key Steps:**\n",
    "\n",
    "1. **Initialization:**\n",
    "   For the first word, compute log-probabilities combining the initial tag probabilities (`pi`) and emission probabilities (`B`).\n",
    "\n",
    "2. **Recursion:**\n",
    "   For each subsequent word, compute the best score for each tag by considering all possible previous tags. Store both the score (`viterbi`) and the best predecessor tag (`path`) for backtracking.\n",
    "\n",
    "   $$\\text{score(tag)} = \\max_{\\text{prev\\_tag}} \\Big( \\text{viterbi}[pos-1][\\text{prev\\_tag}] + \\log P(\\text{tag} \\mid \\text{prev\\_tag}) + \\log P(\\text{word} \\mid \\text{tag})\\Big)$$\n",
    "\n",
    "3. **Termination / Backtracking:**\n",
    "   * Start from the tag with the highest score at the last word.\n",
    "   * Backtrack using `path` to reconstruct the optimal tag sequence.\n",
    "\n",
    "**Advantages:**\n",
    "* Guarantees finding the globally optimal sequence.\n",
    "* Much more efficient than brute-force decoding: $O(n \\cdot t^2)$ instead of $O(t^n)$.\n",
    "\n",
    "**Unknown Words Handling:**\n",
    "* Words not seen during training are assigned a small probability (`1e-6`) to avoid zero probabilities."
   ],
   "metadata": {
    "id": "lpNfeoPyhF53"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def viterbi_decode(sentence, tag_set, pi, A, B):\n",
    "    n = len(sentence)\n",
    "    viterbi = []\n",
    "    path = []\n",
    "\n",
    "    # Initialization\n",
    "    initial_scores = {}\n",
    "    initial_pointers = {}\n",
    "    for tag in tag_set:\n",
    "        emission_val = B[tag].get(sentence[0], 1e-6)\n",
    "        initial_scores[tag] = math.log(pi[tag]) + math.log(emission_val)\n",
    "        initial_pointers[tag] = None\n",
    "\n",
    "    viterbi.append(initial_scores)\n",
    "    path.append(initial_pointers)\n",
    "\n",
    "    # Recursion\n",
    "    for pos in range(1, n):\n",
    "        current_scores = {}\n",
    "        current_pointers = {}\n",
    "        word = sentence[pos]\n",
    "\n",
    "        for tag in tag_set:\n",
    "            best_score = float(\"-inf\")\n",
    "            best_predecessor = None\n",
    "\n",
    "            for prev_tag in tag_set:\n",
    "                trans_prob = A[prev_tag][tag]\n",
    "                emit_prob = B[tag].get(word, 1e-6)\n",
    "                score = viterbi[pos - 1][prev_tag] + math.log(trans_prob) + math.log(emit_prob)\n",
    "\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_predecessor = prev_tag\n",
    "\n",
    "            current_scores[tag] = best_score\n",
    "            current_pointers[tag] = best_predecessor\n",
    "\n",
    "        viterbi.append(current_scores)\n",
    "        path.append(current_pointers)\n",
    "\n",
    "    # Termination: backtrack to find the best sequence\n",
    "    final_tag = max(viterbi[-1], key=viterbi[-1].get)\n",
    "    result = [final_tag]\n",
    "    for pos in range(n - 1, 0, -1):\n",
    "        result.append(path[pos][result[-1]])\n",
    "\n",
    "    result.reverse()\n",
    "    return result"
   ],
   "metadata": {
    "id": "lEZIn_xMctZz"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**11. Train the Model**",
   "metadata": {
    "id": "9XvAq5tZaSQx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialize the tagger and train it on the training corpus."
   ],
   "metadata": {
    "id": "-7dSF_9LaYB6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "hmm_tagger = HMMPOSTagger()\n",
    "hmm_tagger.train(training_data)\n",
    "print(\"Model training complete!\")\n",
    "print(f\"Tags in model: {len(hmm_tagger.tag_set)}\")\n",
    "print(f\"Vocabulary size: {len(hmm_tagger.word_set)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9N99w7JsaTxF",
    "outputId": "82e92b2f-adff-478e-b3d1-cc4d1432d506"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model training complete!\n",
      "Tags in model: 5\n",
      "Vocabulary size: 19\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**12. Test the Tagger**",
   "metadata": {
    "id": "99nzwUBqaeVW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluate the tagger on test sentences using both algorithms.\n",
    "Both methods should produce identical results."
   ],
   "metadata": {
    "id": "ayEsCcZ7a_vq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_sentences = [\n",
    "    [\"Small\", \"cat\", \"sleeps\"],\n",
    "    [\"The\", \"young\", \"girl\", \"dances\", \"gracefully\"],\n",
    "    [\"The\", \"moon\", \"glows\"],\n",
    "    [\"A\", \"tiny\", \"rabbit\", \"nibbles\", \"carrots\"],\n",
    "    [\"The\", \"cheerful\", \"child\", \"plays\", \"outside\"],\n",
    "    [\"Dogs\", \"run\", \"fast\"],\n",
    "    [\"The\", \"boy\", \"writes\", \"a\", \"letter\"],\n",
    "    [\"A\", \"colorful\", \"butterfly\", \"flies\", \"away\"]\n",
    "]\n",
    "\n",
    "for sent in test_sentences:\n",
    "    brute_tags = hmm_tagger.brute_force_decode(sent)\n",
    "    viterbi_tags = viterbi_decode(sent, hmm_tagger.tag_set, hmm_tagger.pi,\n",
    "                                  hmm_tagger.A, hmm_tagger.B)\n",
    "\n",
    "    print(f\"Sentence: {' '.join(sent)}\")\n",
    "    print(f\"  Brute Force: {', '.join(brute_tags)}\")\n",
    "    print(f\"  Viterbi:     {', '.join(viterbi_tags)}\\n\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F5Z7sIw2aeqw",
    "outputId": "6ff4b7ff-589e-4ab4-c848-899517e96ddd"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence: Small cat sleeps\n",
      "  Brute Force: DET, NOUN, VERB\n",
      "  Viterbi:     DET, NOUN, VERB\n",
      "\n",
      "Sentence: The young girl dances gracefully\n",
      "  Brute Force: DET, NOUN, ADP, DET, NOUN\n",
      "  Viterbi:     DET, NOUN, ADP, DET, NOUN\n",
      "\n",
      "Sentence: The moon glows\n",
      "  Brute Force: DET, NOUN, VERB\n",
      "  Viterbi:     DET, NOUN, VERB\n",
      "\n",
      "Sentence: A tiny rabbit nibbles carrots\n",
      "  Brute Force: DET, NOUN, ADP, DET, NOUN\n",
      "  Viterbi:     DET, NOUN, ADP, DET, NOUN\n",
      "\n",
      "Sentence: The cheerful child plays outside\n",
      "  Brute Force: DET, NOUN, ADP, DET, NOUN\n",
      "  Viterbi:     DET, NOUN, ADP, DET, NOUN\n",
      "\n",
      "Sentence: Dogs run fast\n",
      "  Brute Force: DET, NOUN, VERB\n",
      "  Viterbi:     DET, NOUN, VERB\n",
      "\n",
      "Sentence: The boy writes a letter\n",
      "  Brute Force: DET, NOUN, ADP, DET, NOUN\n",
      "  Viterbi:     DET, NOUN, ADP, DET, NOUN\n",
      "\n",
      "Sentence: A colorful butterfly flies away\n",
      "  Brute Force: DET, NOUN, ADP, DET, NOUN\n",
      "  Viterbi:     DET, NOUN, ADP, DET, NOUN\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**13. Complexity Analysis (Bonus)**",
   "metadata": {
    "id": "oVBgOckutDiP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TIME COMPLEXITY COMPARISON**\n",
    "\n",
    "**1. BRUTE FORCE method:**\n",
    "   * Generates all possible tag sequences\n",
    "   * If sentence has n words and t tags exist:\n",
    "     Time complexity: O(t^n × n)\n",
    "\n",
    "     * t^n sequences to generate\n",
    "     * n operations for each sequence\n",
    "\n",
    "   Example: 5 tags, 10 words → 5^10 = 9,765,625 sequences\n",
    "\n",
    "   Problem: Exponential growth - impractical for long sentences\n",
    "\n",
    "**2. VITERBI algorithm (Dynamic Programming):**\n",
    "\n",
    "   * Uses dynamic programming to store solutions to subproblems\n",
    "   * Time complexity: O(n × t^2)\n",
    "\n",
    "     * n positions in sentence\n",
    "     * t^2 transitions for each position\n",
    "\n",
    "   Example: 5 tags, 10 words → 10 × 5^2 = 250 operations\n",
    "\n",
    "   Advantage: Multiple orders of magnitude faster for long sentences\n",
    "\n",
    "**SUMMARY:**\n",
    "\n",
    "* Brute Force: O(t^n × n) - Exponential, but easy to understand\n",
    "* Viterbi: O(n × t^2) - Polynomial, efficient\n",
    "* Viterbi is always used in real POS tagging systems\n"
   ],
   "metadata": {
    "id": "POgoXfTb1EGJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPLEXITY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Parameters\n",
    "n_words = 10\n",
    "n_tags = len(hmm_tagger.tag_set)\n",
    "\n",
    "# Compute operations\n",
    "brute_force_ops = n_tags ** n_words * n_words\n",
    "viterbi_ops = n_words * n_tags ** 2\n",
    "\n",
    "# Print parameters\n",
    "print(f\"\\nParameters:\")\n",
    "print(f\"  - Number of words (n): {n_words}\")\n",
    "print(f\"  - Number of tags (t): {n_tags}\")\n",
    "\n",
    "# Brute Force Complexity\n",
    "print(f\"\\nBrute Force - O(t^n × n):\")\n",
    "print(f\"  Operations: {brute_force_ops:,}\")\n",
    "\n",
    "# Viterbi Complexity\n",
    "print(f\"\\nViterbi - O(n × t^2):\")\n",
    "print(f\"  Operations: {viterbi_ops:,}\")\n",
    "\n",
    "# Speedup ratio\n",
    "speedup = brute_force_ops / viterbi_ops\n",
    "print(f\"\\nRatio: Viterbi is {speedup:,.0f}x faster\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROGRAM COMPLETED\")\n",
    "print(\"=\"*60)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qQsKIw9ItBg6",
    "outputId": "2d46cb80-0424-4899-a3c2-9199c01143bf"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "============================================================\n",
      "COMPLEXITY ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Parameters:\n",
      "  - Number of words (n): 10\n",
      "  - Number of tags (t): 5\n",
      "\n",
      "Brute Force - O(t^n × n):\n",
      "  Operations: 97,656,250\n",
      "\n",
      "Viterbi - O(n × t^2):\n",
      "  Operations: 250\n",
      "\n",
      "Ratio: Viterbi is 390,625x faster\n",
      "\n",
      "============================================================\n",
      "PROGRAM COMPLETED\n",
      "============================================================\n"
     ]
    }
   ]
  }
 ]
}
